{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import walk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from parse_data import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_rows', 9000)\n",
    "pd.set_option('display.max_columns', 1500)\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = '..'\n",
    "\n",
    "resultpath = main_folder_path + '/output_full_dataset/'\n",
    "\n",
    "gold_standard_path = 'result_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Gold standard (## 1000Genomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 2014 dataset\n",
    "\n",
    "Loading and preprocessing of gold-standard dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "milleg_2014_df = pd.read_csv('../reference_data/1000G_hla_diversity_2014.txt', sep = \" \", comment='#')\n",
    "\n",
    "#Remove quotes\n",
    "#Change name of Utah individuals from CEPH to CEU as seen in the 1000 genomes database:\n",
    "#Remove samples with NaN (non typed alleles)\n",
    "mille_gs_df = milleg_2014_df.replace({'\\\"':''}, regex=True).replace('CEPH','CEU').dropna()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove samples, which doesn't have exome data on 1000Genomes and create download links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def checkUrl(url):\n",
    "    p = urlparse(url)\n",
    "    conn = http.client.HTTPConnection(p.netloc)\n",
    "    conn.request('HEAD', p.path)\n",
    "    resp = conn.getresponse()\n",
    "    return resp.status < 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The results of this code are saved in two files, to save time\n",
    "\n",
    "# #Check all exome links to create a list of id's with exome data\n",
    "# gold_standard_id_list = list()\n",
    "\n",
    "# #Save the valid urls for download later\n",
    "# gold_standard_url_list = list()\n",
    "\n",
    "# i = 0\n",
    "\n",
    "# original_sample_list = list(mille_gs_df['id'])\n",
    "\n",
    "# for identity in original_sample_list:\n",
    "#     i += 1\n",
    "#     if i % 100 == 0:\n",
    "#         print(i, \" iterations completed\")\n",
    "        \n",
    "#     sbgroup = mille_gs_df[mille_gs_df['id'] == identity]['sbgroup'].iloc[0]         \n",
    "#     wget_url = \"ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/{}/{}/exome_alignment/{}.alt_bwamem_GRCh38DH.20150826.{}.exome.cram\".format(sbgroup, identity, identity, sbgroup, )\n",
    "    \n",
    "#     if checkUrl(wget_url):\n",
    "#         gold_standard_id_list.append(identity)\n",
    "#         gold_standard_url_list.append(wget_url)\n",
    "\n",
    "# with open('../reference_data/gold_standard_url_list.txt', 'w') as outfile:\n",
    "#     for entry in wget_url_list:\n",
    "#         outfile.write(entry + '\\n')\n",
    "    \n",
    "# with open('../reference_data/gold_standard_id_list.txt', 'w') as outfile:\n",
    "#     for entry in valid_wes_id_list:\n",
    "#         outfile.write(entry + '\\n')\n",
    "\n",
    "gold_standard_url_list = list()\n",
    "with open('../reference_data/gold_standard_url_list.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        gold_standard_url_list.append(line[:-1])   \n",
    "\n",
    "gold_standard_id_list= list()\n",
    "with open('../reference_data/gold_standard_id_list.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        gold_standard_id_list.append(line[:-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MG_exome_df = mille_gs_df[mille_gs_df['id'].isin(gold_standard_id_list)].drop('sbgroup', axis = 1)\n",
    "\n",
    "#Replace 0000 with empty string:\n",
    "MG_exome_df.replace('0000', '', inplace=True)\n",
    "\n",
    "#MG_exome_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Typing\n",
    "Merge rows, where a person has been typed twice. (this was only relevant for 2014 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ids: 819\n",
      "Number of total ids: 822\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>A</th>\n",
       "      <th>A.1</th>\n",
       "      <th>B</th>\n",
       "      <th>B.1</th>\n",
       "      <th>C</th>\n",
       "      <th>C.1</th>\n",
       "      <th>DRB1</th>\n",
       "      <th>DRB1.1</th>\n",
       "      <th>DQB1</th>\n",
       "      <th>DQB1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>NA19119</td>\n",
       "      <td>23:01:01/23:07N/23:17/23:18/23:20</td>\n",
       "      <td>36:01</td>\n",
       "      <td>35:01:01/35:01:03/35:40N/35:42/35:57/35:94</td>\n",
       "      <td>49:01:01</td>\n",
       "      <td>04:01:01:01/04:01:01:02/04:01:01:03/04:09N/04:28/04:30/04:41</td>\n",
       "      <td>07:01:01/07:01:02/07:01:09/07:06/07:18/07:52</td>\n",
       "      <td>03:01:01:01/03:01:01:02</td>\n",
       "      <td>07:01:01:01/07:01:01:02</td>\n",
       "      <td>02:01:01/02:02/02:04</td>\n",
       "      <td>05:01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>NA19119</td>\n",
       "      <td>23:01</td>\n",
       "      <td>36:01</td>\n",
       "      <td>35:01</td>\n",
       "      <td>49:01</td>\n",
       "      <td>04:01</td>\n",
       "      <td>07:01</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>NA19210</td>\n",
       "      <td>03:01</td>\n",
       "      <td>33:01</td>\n",
       "      <td>15:10</td>\n",
       "      <td>58:01</td>\n",
       "      <td>03:02:01/03:02:02/03:02:03</td>\n",
       "      <td>08:04</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>NA19210</td>\n",
       "      <td>03:01:01:01/03:01:01:02N/03:01:01:03/03:01:07/03:20/03:21N/03:26/03:37/03:45</td>\n",
       "      <td>33:01:01</td>\n",
       "      <td>15:10</td>\n",
       "      <td>58:01:01/58:11</td>\n",
       "      <td>03:02:01/03:02:02/03:02:03</td>\n",
       "      <td>08:04</td>\n",
       "      <td>03:01:01:01/03:01:01:02</td>\n",
       "      <td>13:01:01</td>\n",
       "      <td>03:01:01/03:01:04/03:09/03:19/03:21/03:22/03:24</td>\n",
       "      <td>06:03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>NA19223</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>03:02</td>\n",
       "      <td>13:27</td>\n",
       "      <td>04:02</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>NA19223</td>\n",
       "      <td>30:01:01/30:01:02/30:24</td>\n",
       "      <td>33:03:01/33:03:03/33:15/33:25</td>\n",
       "      <td>41:04</td>\n",
       "      <td>42:01</td>\n",
       "      <td>17:01/17:02/17:03</td>\n",
       "      <td>17:01/17:02/17:03</td>\n",
       "      <td>03:02:01</td>\n",
       "      <td>13:27</td>\n",
       "      <td>02:01:01/02:02/02:04</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  \\\n",
       "440  NA19119   \n",
       "441  NA19119   \n",
       "480  NA19210   \n",
       "481  NA19210   \n",
       "486  NA19223   \n",
       "487  NA19223   \n",
       "\n",
       "                                                                                A  \\\n",
       "440                                             23:01:01/23:07N/23:17/23:18/23:20   \n",
       "441                                                                         23:01   \n",
       "480                                                                         03:01   \n",
       "481  03:01:01:01/03:01:01:02N/03:01:01:03/03:01:07/03:20/03:21N/03:26/03:37/03:45   \n",
       "486                                                                                 \n",
       "487                                                       30:01:01/30:01:02/30:24   \n",
       "\n",
       "                               A.1  \\\n",
       "440                          36:01   \n",
       "441                          36:01   \n",
       "480                          33:01   \n",
       "481                       33:01:01   \n",
       "486                                  \n",
       "487  33:03:01/33:03:03/33:15/33:25   \n",
       "\n",
       "                                              B             B.1  \\\n",
       "440  35:01:01/35:01:03/35:40N/35:42/35:57/35:94        49:01:01   \n",
       "441                                       35:01           49:01   \n",
       "480                                       15:10           58:01   \n",
       "481                                       15:10  58:01:01/58:11   \n",
       "486                                                               \n",
       "487                                       41:04           42:01   \n",
       "\n",
       "                                                                C  \\\n",
       "440  04:01:01:01/04:01:01:02/04:01:01:03/04:09N/04:28/04:30/04:41   \n",
       "441                                                         04:01   \n",
       "480                                    03:02:01/03:02:02/03:02:03   \n",
       "481                                    03:02:01/03:02:02/03:02:03   \n",
       "486                                                                 \n",
       "487                                             17:01/17:02/17:03   \n",
       "\n",
       "                                              C.1                     DRB1  \\\n",
       "440  07:01:01/07:01:02/07:01:09/07:06/07:18/07:52  03:01:01:01/03:01:01:02   \n",
       "441                                         07:01                            \n",
       "480                                         08:04                            \n",
       "481                                         08:04  03:01:01:01/03:01:01:02   \n",
       "486                                                                  03:02   \n",
       "487                             17:01/17:02/17:03                 03:02:01   \n",
       "\n",
       "                      DRB1.1                                             DQB1  \\\n",
       "440  07:01:01:01/07:01:01:02                             02:01:01/02:02/02:04   \n",
       "441                                                                             \n",
       "480                                                                             \n",
       "481                 13:01:01  03:01:01/03:01:04/03:09/03:19/03:21/03:22/03:24   \n",
       "486                    13:27                                            04:02   \n",
       "487                    13:27                             02:01:01/02:02/02:04   \n",
       "\n",
       "       DQB1.1  \n",
       "440  05:01:01  \n",
       "441            \n",
       "480            \n",
       "481  06:03:01  \n",
       "486     02:01  \n",
       "487     04:02  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for non-identical rows\n",
    "print(f\"Number of unique ids: {len(set(list(MG_exome_df['id'])))}\")\n",
    "print(f\"Number of total ids: {len(list(MG_exome_df['id']))}\")\n",
    "\n",
    "non_unique = list({x for x in list(MG_exome_df['id']) if list(MG_exome_df['id']).count(x) > 1})\n",
    "non_unique_df = MG_exome_df[MG_exome_df['id'].isin(non_unique)]\n",
    "non_unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1840502/2213804282.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_unique_df.loc[486,'DQB1'] = '02:01'\n",
      "/tmp/ipykernel_1840502/2213804282.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_unique_df.loc[486,'DQB1.1'] = '04:02'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>A</th>\n",
       "      <th>A.1</th>\n",
       "      <th>B</th>\n",
       "      <th>B.1</th>\n",
       "      <th>C</th>\n",
       "      <th>C.1</th>\n",
       "      <th>DRB1</th>\n",
       "      <th>DRB1.1</th>\n",
       "      <th>DQB1</th>\n",
       "      <th>DQB1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>NA19119</td>\n",
       "      <td>23:01:01/23:07N/23:17/23:18/23:20</td>\n",
       "      <td>36:01</td>\n",
       "      <td>35:01:01/35:01:03/35:40N/35:42/35:57/35:94</td>\n",
       "      <td>49:01:01</td>\n",
       "      <td>04:01:01:01/04:01:01:02/04:01:01:03/04:09N/04:28/04:30/04:41</td>\n",
       "      <td>07:01:01/07:01:02/07:01:09/07:06/07:18/07:52</td>\n",
       "      <td>03:01:01:01/03:01:01:02</td>\n",
       "      <td>07:01:01:01/07:01:01:02</td>\n",
       "      <td>02:01:01/02:02/02:04</td>\n",
       "      <td>05:01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>NA19119</td>\n",
       "      <td>23:01</td>\n",
       "      <td>36:01</td>\n",
       "      <td>35:01</td>\n",
       "      <td>49:01</td>\n",
       "      <td>04:01</td>\n",
       "      <td>07:01</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>NA19210</td>\n",
       "      <td>03:01</td>\n",
       "      <td>33:01</td>\n",
       "      <td>15:10</td>\n",
       "      <td>58:01</td>\n",
       "      <td>03:02:01/03:02:02/03:02:03</td>\n",
       "      <td>08:04</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>NA19210</td>\n",
       "      <td>03:01:01:01/03:01:01:02N/03:01:01:03/03:01:07/03:20/03:21N/03:26/03:37/03:45</td>\n",
       "      <td>33:01:01</td>\n",
       "      <td>15:10</td>\n",
       "      <td>58:01:01/58:11</td>\n",
       "      <td>03:02:01/03:02:02/03:02:03</td>\n",
       "      <td>08:04</td>\n",
       "      <td>03:01:01:01/03:01:01:02</td>\n",
       "      <td>13:01:01</td>\n",
       "      <td>03:01:01/03:01:04/03:09/03:19/03:21/03:22/03:24</td>\n",
       "      <td>06:03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>NA19223</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>03:02</td>\n",
       "      <td>13:27</td>\n",
       "      <td>02:01</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>NA19223</td>\n",
       "      <td>30:01:01/30:01:02/30:24</td>\n",
       "      <td>33:03:01/33:03:03/33:15/33:25</td>\n",
       "      <td>41:04</td>\n",
       "      <td>42:01</td>\n",
       "      <td>17:01/17:02/17:03</td>\n",
       "      <td>17:01/17:02/17:03</td>\n",
       "      <td>03:02:01</td>\n",
       "      <td>13:27</td>\n",
       "      <td>02:01:01/02:02/02:04</td>\n",
       "      <td>04:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  \\\n",
       "440  NA19119   \n",
       "441  NA19119   \n",
       "480  NA19210   \n",
       "481  NA19210   \n",
       "486  NA19223   \n",
       "487  NA19223   \n",
       "\n",
       "                                                                                A  \\\n",
       "440                                             23:01:01/23:07N/23:17/23:18/23:20   \n",
       "441                                                                         23:01   \n",
       "480                                                                         03:01   \n",
       "481  03:01:01:01/03:01:01:02N/03:01:01:03/03:01:07/03:20/03:21N/03:26/03:37/03:45   \n",
       "486                                                                                 \n",
       "487                                                       30:01:01/30:01:02/30:24   \n",
       "\n",
       "                               A.1  \\\n",
       "440                          36:01   \n",
       "441                          36:01   \n",
       "480                          33:01   \n",
       "481                       33:01:01   \n",
       "486                                  \n",
       "487  33:03:01/33:03:03/33:15/33:25   \n",
       "\n",
       "                                              B             B.1  \\\n",
       "440  35:01:01/35:01:03/35:40N/35:42/35:57/35:94        49:01:01   \n",
       "441                                       35:01           49:01   \n",
       "480                                       15:10           58:01   \n",
       "481                                       15:10  58:01:01/58:11   \n",
       "486                                                               \n",
       "487                                       41:04           42:01   \n",
       "\n",
       "                                                                C  \\\n",
       "440  04:01:01:01/04:01:01:02/04:01:01:03/04:09N/04:28/04:30/04:41   \n",
       "441                                                         04:01   \n",
       "480                                    03:02:01/03:02:02/03:02:03   \n",
       "481                                    03:02:01/03:02:02/03:02:03   \n",
       "486                                                                 \n",
       "487                                             17:01/17:02/17:03   \n",
       "\n",
       "                                              C.1                     DRB1  \\\n",
       "440  07:01:01/07:01:02/07:01:09/07:06/07:18/07:52  03:01:01:01/03:01:01:02   \n",
       "441                                         07:01                            \n",
       "480                                         08:04                            \n",
       "481                                         08:04  03:01:01:01/03:01:01:02   \n",
       "486                                                                  03:02   \n",
       "487                             17:01/17:02/17:03                 03:02:01   \n",
       "\n",
       "                      DRB1.1                                             DQB1  \\\n",
       "440  07:01:01:01/07:01:01:02                             02:01:01/02:02/02:04   \n",
       "441                                                                             \n",
       "480                                                                             \n",
       "481                 13:01:01  03:01:01/03:01:04/03:09/03:19/03:21/03:22/03:24   \n",
       "486                    13:27                                            02:01   \n",
       "487                    13:27                             02:01:01/02:02/02:04   \n",
       "\n",
       "       DQB1.1  \n",
       "440  05:01:01  \n",
       "441            \n",
       "480            \n",
       "481  06:03:01  \n",
       "486     04:02  \n",
       "487     04:02  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DQB1 and DQB1.1 are not sorted in order between for both entries of NA19223 - switch these around for one entry, so they match columnwise\n",
    "non_unique_df.loc[486,'DQB1'] = '02:01'\n",
    "non_unique_df.loc[486,'DQB1.1'] = '04:02'\n",
    "\n",
    "non_unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_duplicates_df = pd.DataFrame()\n",
    "for column in ['A','A.1','B','B.1','C','C.1','DRB1','DRB1.1','DQB1','DQB1.1']:\n",
    "    clean_duplicates_df[column] = non_unique_df.groupby(['id'])[column].apply('/'.join)\n",
    "    \n",
    "    #Remove potentaial starting '/'\n",
    "    for identity in clean_duplicates_df.index:\n",
    "        entry = clean_duplicates_df.loc[identity, column]\n",
    "        \n",
    "        if entry.startswith('/'):\n",
    "            clean_duplicates_df.at[identity, column] = entry[1:]\n",
    "        if entry.endswith('/'):\n",
    "            clean_duplicates_df.at[identity, column] = entry[:-1]\n",
    "        \n",
    "clean_duplicates_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>A</th>\n",
       "      <th>A.1</th>\n",
       "      <th>B</th>\n",
       "      <th>B.1</th>\n",
       "      <th>C</th>\n",
       "      <th>C.1</th>\n",
       "      <th>DRB1</th>\n",
       "      <th>DRB1.1</th>\n",
       "      <th>DQB1</th>\n",
       "      <th>DQB1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA19119</td>\n",
       "      <td>23:01:01/23:07N/23:17/23:18/23:20/23:01</td>\n",
       "      <td>36:01/36:01</td>\n",
       "      <td>35:01:01/35:01:03/35:40N/35:42/35:57/35:94/35:01</td>\n",
       "      <td>49:01:01/49:01</td>\n",
       "      <td>04:01:01:01/04:01:01:02/04:01:01:03/04:09N/04:28/04:30/04:41/04:01</td>\n",
       "      <td>07:01:01/07:01:02/07:01:09/07:06/07:18/07:52/07:01</td>\n",
       "      <td>03:01:01:01/03:01:01:02</td>\n",
       "      <td>07:01:01:01/07:01:01:02</td>\n",
       "      <td>02:01:01/02:02/02:04</td>\n",
       "      <td>05:01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NA19210</td>\n",
       "      <td>03:01/03:01:01:01/03:01:01:02N/03:01:01:03/03:01:07/03:20/03:21N/03:26/03:37/03:45</td>\n",
       "      <td>33:01/33:01:01</td>\n",
       "      <td>15:10/15:10</td>\n",
       "      <td>58:01/58:01:01/58:11</td>\n",
       "      <td>03:02:01/03:02:02/03:02:03/03:02:01/03:02:02/03:02:03</td>\n",
       "      <td>08:04/08:04</td>\n",
       "      <td>03:01:01:01/03:01:01:02</td>\n",
       "      <td>13:01:01</td>\n",
       "      <td>03:01:01/03:01:04/03:09/03:19/03:21/03:22/03:24</td>\n",
       "      <td>06:03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NA19223</td>\n",
       "      <td>30:01:01/30:01:02/30:24</td>\n",
       "      <td>33:03:01/33:03:03/33:15/33:25</td>\n",
       "      <td>41:04</td>\n",
       "      <td>42:01</td>\n",
       "      <td>17:01/17:02/17:03</td>\n",
       "      <td>17:01/17:02/17:03</td>\n",
       "      <td>03:02/03:02:01</td>\n",
       "      <td>13:27/13:27</td>\n",
       "      <td>02:01/02:01:01/02:02/02:04</td>\n",
       "      <td>04:02/04:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  NA19119   \n",
       "1  NA19210   \n",
       "2  NA19223   \n",
       "\n",
       "                                                                                    A  \\\n",
       "0                                             23:01:01/23:07N/23:17/23:18/23:20/23:01   \n",
       "1  03:01/03:01:01:01/03:01:01:02N/03:01:01:03/03:01:07/03:20/03:21N/03:26/03:37/03:45   \n",
       "2                                                             30:01:01/30:01:02/30:24   \n",
       "\n",
       "                             A.1  \\\n",
       "0                    36:01/36:01   \n",
       "1                 33:01/33:01:01   \n",
       "2  33:03:01/33:03:03/33:15/33:25   \n",
       "\n",
       "                                                  B                   B.1  \\\n",
       "0  35:01:01/35:01:03/35:40N/35:42/35:57/35:94/35:01        49:01:01/49:01   \n",
       "1                                       15:10/15:10  58:01/58:01:01/58:11   \n",
       "2                                             41:04                 42:01   \n",
       "\n",
       "                                                                    C  \\\n",
       "0  04:01:01:01/04:01:01:02/04:01:01:03/04:09N/04:28/04:30/04:41/04:01   \n",
       "1               03:02:01/03:02:02/03:02:03/03:02:01/03:02:02/03:02:03   \n",
       "2                                                   17:01/17:02/17:03   \n",
       "\n",
       "                                                  C.1  \\\n",
       "0  07:01:01/07:01:02/07:01:09/07:06/07:18/07:52/07:01   \n",
       "1                                         08:04/08:04   \n",
       "2                                   17:01/17:02/17:03   \n",
       "\n",
       "                      DRB1                   DRB1.1  \\\n",
       "0  03:01:01:01/03:01:01:02  07:01:01:01/07:01:01:02   \n",
       "1  03:01:01:01/03:01:01:02                 13:01:01   \n",
       "2           03:02/03:02:01              13:27/13:27   \n",
       "\n",
       "                                              DQB1       DQB1.1  \n",
       "0                             02:01:01/02:02/02:04     05:01:01  \n",
       "1  03:01:01/03:01:04/03:09/03:19/03:21/03:22/03:24     06:03:01  \n",
       "2                       02:01/02:01:01/02:02/02:04  04:02/04:02  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_duplicates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819\n",
      "819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1840502/429365880.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_df = MG_exome_df.append(clean_duplicates_df, sort=False)\n"
     ]
    }
   ],
   "source": [
    "#Remove the duplicate rows from the full dataframe\"\n",
    "MG_exome_df = MG_exome_df[~MG_exome_df['id'].isin(non_unique)]\n",
    "\n",
    "#Add back the clean duplicate rows:\n",
    "MG_exome_df = MG_exome_df.append(clean_duplicates_df, sort=False)\n",
    "\n",
    "#Reset index\n",
    "MG_exome_df.reset_index(inplace=True, drop = True)\n",
    "\n",
    "#Check that only uniwue entries exist now.\n",
    "print(len(set(list(MG_exome_df['id']))))\n",
    "print(len(list(MG_exome_df['id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set id as index\n",
    "MG_exome_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate multiple predictions to interable lists and mark untyped alleles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non typed samples in 2014:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NA12234', 'NA12249']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remember entries, which are not typed\n",
    "non_typed_samples = list()\n",
    "\n",
    "#Furthermore - in cases, where 2 field resolution is not available, use 2018 data.\n",
    "for identity in list(MG_exome_df.index):\n",
    "    \n",
    "    for col in MG_exome_df.columns:\n",
    "        #get list of predictions\n",
    "        predictions_list = MG_exome_df.loc[identity,col].split('/')\n",
    "        \n",
    "        #Put gene name in front of all entries in the list\n",
    "        for i in range(len(predictions_list)):\n",
    "            predictions_list[i] = col.split('.')[0] + \"*\" + predictions_list[i]\n",
    "        \n",
    "        #Check, that all entries in the list have at least four field resolution:\n",
    "        for pred in predictions_list:\n",
    "            \n",
    "            #Remove all non valid entries\n",
    "            if convert_to_two_field(pred) == None:\n",
    "                predictions_list.remove(pred)\n",
    "        \n",
    "        #Note, if no proper typing was made in 2014\n",
    "        if len(predictions_list) == 0:\n",
    "            non_typed_samples.append(identity)\n",
    "            predictions_list = ['not_typed_in_2014']\n",
    "            \n",
    "        #Convert to P-type resolution:\n",
    "        \n",
    " \n",
    "        MG_exome_df.at[identity,col] = predictions_list\n",
    "\n",
    "print(\"Non typed samples in 2014:\")\n",
    "non_typed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save 2014 clean dataset:\n",
    "\n",
    "MG_exome_df.to_pickle('../reference_data/1000G_2014_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Alleles in original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge haplotypes\n",
    "MG_exome_df['A_merged']= MG_exome_df[['A', 'A.1']].apply(lambda x: list(x), axis=1)\n",
    "MG_exome_df['B_merged']= MG_exome_df[['B', 'B.1']].apply(lambda x: list(x), axis=1)\n",
    "MG_exome_df['C_merged']= MG_exome_df[['C', 'C.1']].apply(lambda x: list(x), axis=1)\n",
    "MG_exome_df['DRB1_merged']= MG_exome_df[['DRB1', 'DRB1.1']].apply(lambda x: list(x), axis=1)\n",
    "MG_exome_df['DQB1_merged']= MG_exome_df[['DQB1', 'DQB1.1']].apply(lambda x: list(x), axis=1)\n",
    "\n",
    "MG_exome_merged_df = MG_exome_df.drop(columns=['A', 'A.1', 'B', 'B.1', 'C', 'C.1', 'DRB1', 'DRB1.1','DQB1', 'DQB1.1'])\n",
    "\n",
    "MG_exome_merged_df.rename(columns={\"A_merged\": \"A\", \"B_merged\": \"B\", \"C_merged\": \"C\", \"DRB1_merged\": \"DRB1\", \"DQB1_merged\": \"DQB1\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and convert 2018 data for error correction in 2014 dataset\n",
    "\n",
    "This data is not used itself directly, but fills out the gaps in the 2014 dataset loaded later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 samples analysed in ATHLATES and with results taken from 2018 dataset.\n",
    "ATHLATES_samples = [\"HG01756\", \"HG01757\", \"HG01872\", \"HG01873\", \"HG01886\", \"HG01953\", \"HG01968\", \"HG02014\", \"HG02057\", \"NA20313\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1840502/1275223798.py:36: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  corrections_2018_df_raw = gs_2018_df.loc[set(changed_sample_indexes)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_2018_filepath = '../reference_data/2018_1129_HLA_types_full_1000_Genomes_Project_panel.txt'\n",
    "\n",
    "gs_2018_raw_df = pd.read_csv(gs_2018_filepath, sep = \"\\t\", comment=\"#\")\n",
    "\n",
    "#Rename columns\n",
    "gs_2018_raw_df.rename(columns={'Sample ID': 'id', 'Population': 'sbgroup', 'HLA-A 1': 'A', 'HLA-A 2': 'A.1', 'HLA-B 1': 'B',\n",
    "                            'HLA-B 2': 'B.1', 'HLA-C 1': 'C', 'HLA-C 2': 'C.1', 'HLA-DQB1 1': 'DQB1', 'HLA-DQB1 2': 'DQB1.1',\n",
    "                            'HLA-DRB1 1': 'DRB1' , 'HLA-DRB1 2': 'DRB1.1' }, inplace=True)\n",
    "\n",
    "#Set id as index\n",
    "gs_2018_raw_df.set_index('id', inplace=True)\n",
    "\n",
    "#Remove samples with NaN (non typed alleles)\n",
    "#gs_2018_df = gs_2018_raw_df.dropna()  \n",
    "gs_2018_df = gs_2018_raw_df        \n",
    "\n",
    "#Find entries with an *, indicating a former mistake in the 2014 dataset\n",
    "changed_sample_indexes = list()\n",
    "\n",
    "for index in gs_2018_df.index:\n",
    "    for col in gs_2018_df.columns[1:]:\n",
    "        if isinstance(gs_2018_df.loc[index][col], float):\n",
    "            if math.isnan(gs_2018_df.loc[index][col]):\n",
    "                continue\n",
    "        for entry in gs_2018_df.loc[index][col]:\n",
    "            if '*' in entry:              \n",
    "                changed_sample_indexes.append(index)\n",
    "\n",
    "#Include samples, which are missing as well.\n",
    "changed_sample_indexes += non_typed_samples\n",
    "\n",
    "#Include the 10 high coverage samples:\n",
    "changed_sample_indexes += ATHLATES_samples\n",
    "\n",
    "#make relevant dataframe smaller\n",
    "corrections_2018_df_raw = gs_2018_df.loc[set(changed_sample_indexes)]\n",
    "\n",
    "#Limit the 2018 dataframe to 2014 samples and the 10 high coverage samples \n",
    "corrections_2018_df = corrections_2018_df_raw[corrections_2018_df_raw.index.isin(list(MG_exome_df.index) + ATHLATES_samples)]\n",
    "\n",
    "#Make dataframe for writing into 2014 dataframe:\n",
    "\n",
    "#Remove * from entry:\n",
    "corrections_2018_df_no_asterix = corrections_2018_df.copy()\n",
    "corrections_2018_df_no_asterix=corrections_2018_df_no_asterix.replace({'\\*':''}, regex=True)\n",
    "\n",
    "#Add allele name in front of the resolution:\n",
    "for identity in corrections_2018_df_no_asterix.index:\n",
    "    for col in corrections_2018_df_no_asterix.columns[2:]:\n",
    "        entry = corrections_2018_df_no_asterix.loc[identity][col]\n",
    "        if isinstance(entry, float):\n",
    "            if math.isnan(entry):\n",
    "                continue\n",
    "        entrylist = corrections_2018_df_no_asterix.loc[identity][col].split('/')       \n",
    "        \n",
    "        entrylist = list(set([col.split('.')[0] + \"*\" + i.split('*')[0] for i in entrylist]))\n",
    "        \n",
    "        corrections_2018_df_no_asterix.at[identity,col] = entrylist\n",
    "\n",
    "len(corrections_2018_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>sbgroup</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>DRB1</th>\n",
       "      <th>DQB1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NA11994</th>\n",
       "      <td>EUR</td>\n",
       "      <td>CEU</td>\n",
       "      <td>[[A*01:01], [A*11:01]]</td>\n",
       "      <td>[[B*07:02], [B*51:01]]</td>\n",
       "      <td>[[C*07:02], [C*15:13]]</td>\n",
       "      <td>[[DRB1*04:02], [DRB1*04:04]]</td>\n",
       "      <td>[[DQB1*03:02], [DQB1*03:02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA06986</th>\n",
       "      <td>EUR</td>\n",
       "      <td>CEU</td>\n",
       "      <td>[[A*03:01], [A*32:01]]</td>\n",
       "      <td>[[B*44:03], [B*44:03]]</td>\n",
       "      <td>[[C*04:01], [C*16:01]]</td>\n",
       "      <td>[[DRB1*07:01], [DRB1*07:01]]</td>\n",
       "      <td>[[DQB1*02:02], [DQB1*02:02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA19116</th>\n",
       "      <td>AFR</td>\n",
       "      <td>YRI</td>\n",
       "      <td>[[A*23:01], [A*33:03]]</td>\n",
       "      <td>[[B*07:02], [B*39:24]]</td>\n",
       "      <td>[[C*07:01], [C*07:02]]</td>\n",
       "      <td>[[DRB1*03:02], [DRB1*13:02]]</td>\n",
       "      <td>[[DQB1*04:02], [DQB1*06:09]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA12156</th>\n",
       "      <td>EUR</td>\n",
       "      <td>CEU</td>\n",
       "      <td>[[A*01:01], [A*11:01]]</td>\n",
       "      <td>[[B*50:01], [B*51:01]]</td>\n",
       "      <td>[[C*06:06], [C*15:02]]</td>\n",
       "      <td>[[DRB1*04:07], [DRB1*07:01]]</td>\n",
       "      <td>[[DQB1*02:02], [DQB1*03:01]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG01356</th>\n",
       "      <td>AMR</td>\n",
       "      <td>CLM</td>\n",
       "      <td>[[A*24:02], [A*68:01]]</td>\n",
       "      <td>[[B*35:43], [B*18:01]]</td>\n",
       "      <td>[[C*01:02], [C*05:01]]</td>\n",
       "      <td>[[DRB1*03:01], [DRB1*04:92]]</td>\n",
       "      <td>[[DQB1*02:01], [DQB1*03:02]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Region sbgroup                       A                       B  \\\n",
       "id                                                                       \n",
       "NA11994    EUR     CEU  [[A*01:01], [A*11:01]]  [[B*07:02], [B*51:01]]   \n",
       "NA06986    EUR     CEU  [[A*03:01], [A*32:01]]  [[B*44:03], [B*44:03]]   \n",
       "NA19116    AFR     YRI  [[A*23:01], [A*33:03]]  [[B*07:02], [B*39:24]]   \n",
       "NA12156    EUR     CEU  [[A*01:01], [A*11:01]]  [[B*50:01], [B*51:01]]   \n",
       "HG01356    AMR     CLM  [[A*24:02], [A*68:01]]  [[B*35:43], [B*18:01]]   \n",
       "\n",
       "                              C                          DRB1  \\\n",
       "id                                                              \n",
       "NA11994  [[C*07:02], [C*15:13]]  [[DRB1*04:02], [DRB1*04:04]]   \n",
       "NA06986  [[C*04:01], [C*16:01]]  [[DRB1*07:01], [DRB1*07:01]]   \n",
       "NA19116  [[C*07:01], [C*07:02]]  [[DRB1*03:02], [DRB1*13:02]]   \n",
       "NA12156  [[C*06:06], [C*15:02]]  [[DRB1*04:07], [DRB1*07:01]]   \n",
       "HG01356  [[C*01:02], [C*05:01]]  [[DRB1*03:01], [DRB1*04:92]]   \n",
       "\n",
       "                                 DQB1  \n",
       "id                                     \n",
       "NA11994  [[DQB1*03:02], [DQB1*03:02]]  \n",
       "NA06986  [[DQB1*02:02], [DQB1*02:02]]  \n",
       "NA19116  [[DQB1*04:02], [DQB1*06:09]]  \n",
       "NA12156  [[DQB1*02:02], [DQB1*03:01]]  \n",
       "HG01356  [[DQB1*02:01], [DQB1*03:02]]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge alleles into one column:\n",
    "\n",
    "corrections_2018_df_no_asterix['A_merged']= corrections_2018_df_no_asterix[['A', 'A.1']].apply(lambda x: list(x), axis=1)\n",
    "corrections_2018_df_no_asterix['B_merged']= corrections_2018_df_no_asterix[['B', 'B.1']].apply(lambda x: list(x), axis=1)\n",
    "corrections_2018_df_no_asterix['C_merged']= corrections_2018_df_no_asterix[['C', 'C.1']].apply(lambda x: list(x), axis=1)\n",
    "corrections_2018_df_no_asterix['DRB1_merged']= corrections_2018_df_no_asterix[['DRB1', 'DRB1.1']].apply(lambda x: list(x), axis=1)\n",
    "corrections_2018_df_no_asterix['DQB1_merged']= corrections_2018_df_no_asterix[['DQB1', 'DQB1.1']].apply(lambda x: list(x), axis=1)\n",
    "\n",
    "corrections_2018_merged_df = corrections_2018_df_no_asterix.drop(columns=['A', 'A.1', 'B', 'B.1', 'C', 'C.1', 'DRB1', 'DRB1.1','DQB1', 'DQB1.1'])\n",
    "\n",
    "corrections_2018_merged_df = corrections_2018_merged_df.rename(columns={\"A_merged\": \"A\", \"B_merged\": \"B\", \"C_merged\": \"C\", \"DRB1_merged\": \"DRB1\", \"DQB1_merged\": \"DQB1\"})\n",
    "corrections_2018_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrections_2018_merged_df[[len(str(corrections_2018_merged_df.loc[i]['DQB1'])) < 11 for i in corrections_2018_merged_df.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update samples, where typing was not performed in 2014:\n",
    "\n",
    "for identity in non_typed_samples:\n",
    "    for col in list(MG_exome_merged_df.columns)[1:]:\n",
    "        entry = MG_exome_merged_df.loc[identity,col]\n",
    "        if ['not_typed_in_2014'] in entry:\n",
    "            \n",
    "            column = col.split('.')[0]\n",
    "                \n",
    "            new_type = corrections_2018_merged_df.loc[identity][column]\n",
    "\n",
    "            old_type = MG_exome_merged_df.loc[identity,column]\n",
    "\n",
    "            corrected_typing = list()\n",
    "\n",
    "            for new_allele in list(new_type):\n",
    "                if (new_allele in list(old_type)[0]):\n",
    "                    corrected_typing.append(list(old_type)[0][0])\n",
    "                elif new_allele in list(old_type)[1]:\n",
    "                    corrected_typing.append(list(old_type)[0][1])\n",
    "                else:\n",
    "                    corrected_typing.append(new_allele)\n",
    "\n",
    "            MG_exome_merged_df.at[identity,column] = corrected_typing\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n",
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n",
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n",
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n",
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n",
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n",
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n",
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n",
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n",
      "/tmp/ipykernel_1840502/714003344.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)\n"
     ]
    }
   ],
   "source": [
    "#Perform update of samples, where mistakes have been found\n",
    "\n",
    "changed_sample_indexes = list()\n",
    "\n",
    "#Log changed samples\n",
    "changed_typing_dict = dict()\n",
    "\n",
    "#Loop over the samples, which needs to be corrected\n",
    "for identity in list(corrections_2018_df.index):\n",
    "    \n",
    "    #Loop over relevant columns (genes)\n",
    "    for col in corrections_2018_df.columns[3:]:\n",
    "        \n",
    "        #Loop over alleles\n",
    "        for entry in corrections_2018_df[corrections_2018_df.index == identity][col]:\n",
    "            \n",
    "            if isinstance(entry, float):\n",
    "                if math.isnan(entry):\n",
    "                    continue\n",
    "            #If allele is corrected since 2014 - update it\n",
    "            if '*' in entry:\n",
    "                column = col.split('.')[0]\n",
    "                \n",
    "                new_type = corrections_2018_merged_df.loc[identity][column]\n",
    "                \n",
    "                old_type = MG_exome_merged_df.loc[identity,column]\n",
    "\n",
    "                if identity not in changed_typing_dict:\n",
    "                    changed_typing_dict[identity] = dict()\n",
    "                    changed_typing_dict[identity][col] = (old_type, new_type)\n",
    "                else:\n",
    "                    changed_typing_dict[identity][col] = (old_type, new_type)\n",
    "                \n",
    "                corrected_typing = list()\n",
    "                \n",
    "                for new_allele in list(new_type):\n",
    "                    if (new_allele in list(old_type)[0]):\n",
    "                        corrected_typing.append(list(old_type)[0][0])\n",
    "                    elif new_allele in list(old_type)[1]:\n",
    "                        corrected_typing.append(list(old_type)[0][1])\n",
    "                    else:\n",
    "                        corrected_typing.append(new_allele)\n",
    "                \n",
    "                MG_exome_merged_df.at[identity,column] = corrected_typing\n",
    "            \n",
    "    #Add the 10 high coverage samples to MG_exome_merged\n",
    "    if identity in ATHLATES_samples:\n",
    "        \n",
    "        high_cov_prediction_row = corrections_2018_merged_df.loc[identity][['A','B','C','DRB1','DQB1']]        \n",
    "        MG_exome_merged_df = MG_exome_merged_df.append(high_cov_prediction_row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA19116\n",
      "NA19116\n",
      "NA12156\n",
      "NA19206\n",
      "HG01389\n",
      "NA12005\n",
      "NA12287\n",
      "NA11840\n",
      "NA19129\n",
      "NA19129\n",
      "NA19239\n",
      "NA10847\n",
      "NA11832\n",
      "HG01124\n",
      "NA19159\n",
      "NA18507\n",
      "NA18504\n",
      "NA19138\n",
      "NA19138\n",
      "NA19138\n",
      "NA19093\n",
      "NA12058\n",
      "NA12058\n",
      "NA19171\n",
      "NA19319\n",
      "NA19141\n"
     ]
    }
   ],
   "source": [
    "#Note samples, where the update affects the allele typing in P group resolution\n",
    "num_updated_loci = 0\n",
    "num_identical_p_group = 0\n",
    "num_differing_p_group = 0\n",
    "\n",
    "for sample, locus_dict in changed_typing_dict.items():\n",
    "    for locus, updated_typing in locus_dict.items():\n",
    "        num_updated_loci += 1\n",
    "        (old_type, new_type) = updated_typing\n",
    "        for i in range(2):\n",
    "            overlap_p_group = False\n",
    "            for old_type_allele in sorted(old_type)[i]:\n",
    "                if convert_to_p_group(old_type_allele) == convert_to_p_group(sorted(new_type)[i][0]):\n",
    "                    overlap_p_group = True\n",
    "            \n",
    "            num_identical_p_group += int(overlap_p_group == True)\n",
    "            num_differing_p_group += int(overlap_p_group == False)\n",
    "            if overlap_p_group == False:\n",
    "                print(sample)\n",
    "                # print(old_type)\n",
    "                # print(new_type)\n",
    "                # print(convert_to_p_group(old_type_allele))\n",
    "                # print(convert_to_p_group(new_type[i][0]))\n",
    "                # print()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_identical_p_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_differing_p_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_updated_loci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make URL list for 10 high coverage WES samples (Validated in ATHLATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HG01756', 'HG01757', 'HG01872', 'HG01873', 'HG01886', 'HG01953', 'HG01968', 'HG02014', 'HG02057', 'NA20313']\n"
     ]
    }
   ],
   "source": [
    "print(ATHLATES_samples) \n",
    "\n",
    "high_cov_url_list = list()\n",
    "\n",
    "# for identity in ATHLATES_samples:\n",
    "       \n",
    "#     sbgroup = m2018_gs_df.loc[identity]['sbgroup']\n",
    "#     wget_url = \"ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/{}/{}/exome_alignment/{}.alt_bwamem_GRCh38DH.20150826.{}.exome.cram\".format(sbgroup, identity, identity, sbgroup, )\n",
    "    \n",
    "#     if checkUrl(wget_url):\n",
    "#         high_cov_url_list.append(wget_url)\n",
    "\n",
    "# with open('../reference_data/high_cov_url_list.txt', 'w') as outfile:\n",
    "#     for entry in high_cov_url_list:\n",
    "#         outfile.write(entry + '\\n')\n",
    "    \n",
    "\n",
    "high_cov_url_list = list()\n",
    "with open('../reference_data/high_cov_url_list.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        high_cov_url_list.append(line[:-1])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write config.yaml file for Snakemake workflow (both 2014 data and high cov data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sample_urllist = list(set(gold_standard_url_list)) + high_cov_url_list\n",
    "full_sample_id_list = list(set(gold_standard_id_list)) + ATHLATES_samples\n",
    "\n",
    "configfile_1 = '../Snakemake/config_all.yaml'\n",
    "\n",
    "with open(configfile_1, 'w') as outfile:\n",
    "    outfile.write('sample_urlist:\\n')\n",
    "    for entry in full_sample_urllist:\n",
    "        entry_name = entry.split('/')[9]\n",
    "        outfile.write(\"  \" + entry_name + \": \" +  \"\\\"\" + entry + \"\\\"\" + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>DRB1</th>\n",
       "      <th>DQB1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NA06985</th>\n",
       "      <td>[[A*03:01], [A*02:01]]</td>\n",
       "      <td>[[B*07:02:01], [B*57:01]]</td>\n",
       "      <td>[[C*07:02], [C*06:02:01:01]]</td>\n",
       "      <td>[[DRB1*15:01], [DRB1*15:01]]</td>\n",
       "      <td>[[DQB1*06:02:01], [DQB1*06:02:01]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA06986</th>\n",
       "      <td>[[A*03:01], [A*32:01:01, A*32:01:02]]</td>\n",
       "      <td>[[B*44:03:01, B*44:03:03, B*44:03:04], [B*44:03:01, B*44:03:03, B*44:03:04]]</td>\n",
       "      <td>[[C*04:01], [C*16:01]]</td>\n",
       "      <td>[[DRB1*07:01], [DRB1*07:01]]</td>\n",
       "      <td>[[DQB1*02:02], [DQB1*02:02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA06994</th>\n",
       "      <td>[[A*02:01], [A*32:01:01, A*32:01:02]]</td>\n",
       "      <td>[[B*40:02], [B*08:01]]</td>\n",
       "      <td>[[C*02:02:02], [C*07:01, C*07:06]]</td>\n",
       "      <td>[[DRB1*01:01], [DRB1*04:04]]</td>\n",
       "      <td>[[DQB1*05:01], [DQB1*03:02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA07000</th>\n",
       "      <td>[[A*02:01], [A*68:01:02]]</td>\n",
       "      <td>[[B*44:02], [B*40:01]]</td>\n",
       "      <td>[[C*03:03, C*03:04], [C*07:04]]</td>\n",
       "      <td>[[DRB1*03:01], [DRB1*11:01:01, DRB1*11:01:08]]</td>\n",
       "      <td>[[DQB1*02:01], [DQB1*03:01]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA07037</th>\n",
       "      <td>[[A*30:01], [A*31:01]]</td>\n",
       "      <td>[[B*15:10], [B*40:01]]</td>\n",
       "      <td>[[C*03:04:02], [C*03:04]]</td>\n",
       "      <td>[[DRB1*04:04], [DRB1*13:02]]</td>\n",
       "      <td>[[DQB1*03:02], [DQB1*06:04]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             A  \\\n",
       "id                                               \n",
       "NA06985                 [[A*03:01], [A*02:01]]   \n",
       "NA06986  [[A*03:01], [A*32:01:01, A*32:01:02]]   \n",
       "NA06994  [[A*02:01], [A*32:01:01, A*32:01:02]]   \n",
       "NA07000              [[A*02:01], [A*68:01:02]]   \n",
       "NA07037                 [[A*30:01], [A*31:01]]   \n",
       "\n",
       "                                                                                    B  \\\n",
       "id                                                                                      \n",
       "NA06985                                                     [[B*07:02:01], [B*57:01]]   \n",
       "NA06986  [[B*44:03:01, B*44:03:03, B*44:03:04], [B*44:03:01, B*44:03:03, B*44:03:04]]   \n",
       "NA06994                                                        [[B*40:02], [B*08:01]]   \n",
       "NA07000                                                        [[B*44:02], [B*40:01]]   \n",
       "NA07037                                                        [[B*15:10], [B*40:01]]   \n",
       "\n",
       "                                          C  \\\n",
       "id                                            \n",
       "NA06985        [[C*07:02], [C*06:02:01:01]]   \n",
       "NA06986              [[C*04:01], [C*16:01]]   \n",
       "NA06994  [[C*02:02:02], [C*07:01, C*07:06]]   \n",
       "NA07000     [[C*03:03, C*03:04], [C*07:04]]   \n",
       "NA07037           [[C*03:04:02], [C*03:04]]   \n",
       "\n",
       "                                                   DRB1  \\\n",
       "id                                                        \n",
       "NA06985                    [[DRB1*15:01], [DRB1*15:01]]   \n",
       "NA06986                    [[DRB1*07:01], [DRB1*07:01]]   \n",
       "NA06994                    [[DRB1*01:01], [DRB1*04:04]]   \n",
       "NA07000  [[DRB1*03:01], [DRB1*11:01:01, DRB1*11:01:08]]   \n",
       "NA07037                    [[DRB1*04:04], [DRB1*13:02]]   \n",
       "\n",
       "                                       DQB1  \n",
       "id                                           \n",
       "NA06985  [[DQB1*06:02:01], [DQB1*06:02:01]]  \n",
       "NA06986        [[DQB1*02:02], [DQB1*02:02]]  \n",
       "NA06994        [[DQB1*05:01], [DQB1*03:02]]  \n",
       "NA07000        [[DQB1*02:01], [DQB1*03:01]]  \n",
       "NA07037        [[DQB1*03:02], [DQB1*06:04]]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MG_exome_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all predictions to 2-field resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_two_field_df = MG_exome_merged_df.copy()\n",
    "\n",
    "#Loop over all entries and update the three gold standard dataframes, so they fit the idividual resolution\n",
    "for identity in list(MG_exome_merged_df.index):\n",
    "    for gene in list(MG_exome_merged_df.columns):\n",
    "        \n",
    "        old_pred_list = MG_exome_merged_df.loc[identity,gene]\n",
    "        \n",
    "        gene_pred_two_field = list()\n",
    "    \n",
    "        #Loop over the two alleles\n",
    "        for allele_list in old_pred_list:             \n",
    "            allele_pred_two_field = list()\n",
    "\n",
    "            #Convert each prediction to it's respective correct format:\n",
    "            for allele in allele_list:\n",
    "                allele_pred_two_field.append(convert_to_two_field(allele))\n",
    "                \n",
    "            #Merge the two alleles for each gene in a list\n",
    "            gene_pred_two_field.append(list(set(allele_pred_two_field)))\n",
    "\n",
    "        #Update dataframes wit0h the new predictions\n",
    "        gs_two_field_df.at[identity,gene] = gene_pred_two_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframes as pickle objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MG_exome_merged_df.to_pickle(gold_standard_path + \"MG_exome_merged_df.pkl\")\n",
    "\n",
    "gs_two_field_df.to_pickle(gold_standard_path + \"gs_two_field_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>DRB1</th>\n",
       "      <th>DQB1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HG01872</th>\n",
       "      <td>[[A*11:02], [A*24:07]]</td>\n",
       "      <td>[[B*27:04], [B*39:05]]</td>\n",
       "      <td>[[C*08:01], [C*12:02]]</td>\n",
       "      <td>[[DRB1*08:03], [DRB1*12:02]]</td>\n",
       "      <td>[[DQB1*03:01], [DQB1*06:01]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG01757</th>\n",
       "      <td>[[A*01:01], [A*02:01]]</td>\n",
       "      <td>[[B*18:01], [B*57:01]]</td>\n",
       "      <td>[[C*07:01], [C*07:01]]</td>\n",
       "      <td>[[DRB1*03:01], [DRB1*07:01]]</td>\n",
       "      <td>[[DQB1*02:01], [DQB1*03:03]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG01756</th>\n",
       "      <td>[[A*66:01], [A*30:02]]</td>\n",
       "      <td>[[B*18:01], [B*41:02]]</td>\n",
       "      <td>[[C*05:01], [C*17:03]]</td>\n",
       "      <td>[[DRB1*03:01], [DRB1*03:01]]</td>\n",
       "      <td>[[DQB1*02:01], [DQB1*02:01]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG01953</th>\n",
       "      <td>[[A*02:01], [A*02:11]]</td>\n",
       "      <td>[[B*15:04], [B*35:05]]</td>\n",
       "      <td>[[C*01:02], [C*04:01]]</td>\n",
       "      <td>[[DRB1*04:11], [DRB1*09:01]]</td>\n",
       "      <td>[[DQB1*03:02], [DQB1*03:03]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG01873</th>\n",
       "      <td>[[A*02:03], [A*03:01]]</td>\n",
       "      <td>[[B*35:03], [B*55:02]]</td>\n",
       "      <td>[[C*04:01], [C*12:03]]</td>\n",
       "      <td>[[DRB1*08:02], [DRB1*14:05]]</td>\n",
       "      <td>[[DQB1*04:02], [DQB1*05:03]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NA20313</th>\n",
       "      <td>[[A*03:01], [A*68:02]]</td>\n",
       "      <td>[[B*35:01], [B*53:01]]</td>\n",
       "      <td>[[C*04:01], [C*04:01]]</td>\n",
       "      <td>[[DRB1*08:04], [DRB1*04:05]]</td>\n",
       "      <td>[[DQB1*03:01], [DQB1*03:02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG01968</th>\n",
       "      <td>[[A*02:01], [A*68:01]]</td>\n",
       "      <td>[[B*07:02], [B*40:02]]</td>\n",
       "      <td>[[C*03:04], [C*07:02]]</td>\n",
       "      <td>[[DRB1*01:03], [DRB1*09:01]]</td>\n",
       "      <td>[[DQB1*03:03], [DQB1*05:01]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG02057</th>\n",
       "      <td>[[A*02:03], [A*31:01]]</td>\n",
       "      <td>[[B*48:01], [B*13:01]]</td>\n",
       "      <td>[[C*03:03], [C*03:04]]</td>\n",
       "      <td>[[DRB1*11:01], [DRB1*13:12]]</td>\n",
       "      <td>[[DQB1*03:01], [DQB1*03:01]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG02014</th>\n",
       "      <td>[[A*02:01], [A*36:01]]</td>\n",
       "      <td>[[B*35:01], [B*40:01]]</td>\n",
       "      <td>[[C*03:04], [C*04:01]]</td>\n",
       "      <td>[[DRB1*01:01], [DRB1*15:01]]</td>\n",
       "      <td>[[DQB1*05:01], [DQB1*06:02]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG01886</th>\n",
       "      <td>[[A*30:02], [A*74:01]]</td>\n",
       "      <td>[[B*15:03], [B*57:03]]</td>\n",
       "      <td>[[C*02:10], [C*07:01]]</td>\n",
       "      <td>[[DRB1*11:01], [DRB1*13:02]]</td>\n",
       "      <td>[[DQB1*05:02], [DQB1*06:09]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              A                       B  \\\n",
       "id                                                        \n",
       "HG01872  [[A*11:02], [A*24:07]]  [[B*27:04], [B*39:05]]   \n",
       "HG01757  [[A*01:01], [A*02:01]]  [[B*18:01], [B*57:01]]   \n",
       "HG01756  [[A*66:01], [A*30:02]]  [[B*18:01], [B*41:02]]   \n",
       "HG01953  [[A*02:01], [A*02:11]]  [[B*15:04], [B*35:05]]   \n",
       "HG01873  [[A*02:03], [A*03:01]]  [[B*35:03], [B*55:02]]   \n",
       "NA20313  [[A*03:01], [A*68:02]]  [[B*35:01], [B*53:01]]   \n",
       "HG01968  [[A*02:01], [A*68:01]]  [[B*07:02], [B*40:02]]   \n",
       "HG02057  [[A*02:03], [A*31:01]]  [[B*48:01], [B*13:01]]   \n",
       "HG02014  [[A*02:01], [A*36:01]]  [[B*35:01], [B*40:01]]   \n",
       "HG01886  [[A*30:02], [A*74:01]]  [[B*15:03], [B*57:03]]   \n",
       "\n",
       "                              C                          DRB1  \\\n",
       "id                                                              \n",
       "HG01872  [[C*08:01], [C*12:02]]  [[DRB1*08:03], [DRB1*12:02]]   \n",
       "HG01757  [[C*07:01], [C*07:01]]  [[DRB1*03:01], [DRB1*07:01]]   \n",
       "HG01756  [[C*05:01], [C*17:03]]  [[DRB1*03:01], [DRB1*03:01]]   \n",
       "HG01953  [[C*01:02], [C*04:01]]  [[DRB1*04:11], [DRB1*09:01]]   \n",
       "HG01873  [[C*04:01], [C*12:03]]  [[DRB1*08:02], [DRB1*14:05]]   \n",
       "NA20313  [[C*04:01], [C*04:01]]  [[DRB1*08:04], [DRB1*04:05]]   \n",
       "HG01968  [[C*03:04], [C*07:02]]  [[DRB1*01:03], [DRB1*09:01]]   \n",
       "HG02057  [[C*03:03], [C*03:04]]  [[DRB1*11:01], [DRB1*13:12]]   \n",
       "HG02014  [[C*03:04], [C*04:01]]  [[DRB1*01:01], [DRB1*15:01]]   \n",
       "HG01886  [[C*02:10], [C*07:01]]  [[DRB1*11:01], [DRB1*13:02]]   \n",
       "\n",
       "                                 DQB1  \n",
       "id                                     \n",
       "HG01872  [[DQB1*03:01], [DQB1*06:01]]  \n",
       "HG01757  [[DQB1*02:01], [DQB1*03:03]]  \n",
       "HG01756  [[DQB1*02:01], [DQB1*02:01]]  \n",
       "HG01953  [[DQB1*03:02], [DQB1*03:03]]  \n",
       "HG01873  [[DQB1*04:02], [DQB1*05:03]]  \n",
       "NA20313  [[DQB1*03:01], [DQB1*03:02]]  \n",
       "HG01968  [[DQB1*03:03], [DQB1*05:01]]  \n",
       "HG02057  [[DQB1*03:01], [DQB1*03:01]]  \n",
       "HG02014  [[DQB1*05:01], [DQB1*06:02]]  \n",
       "HG01886  [[DQB1*05:02], [DQB1*06:09]]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_two_field_df[gs_two_field_df.index.isin(ATHLATES_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_e_group_df = MG_exome_merged_df.copy()\n",
    "\n",
    "# #Loop over all entries and update the three gold standard dataframes, so they fit the idividual resolution\n",
    "# for identity in list(MG_exome_merged_df.index):\n",
    "#     for gene in list(MG_exome_merged_df.columns):\n",
    "        \n",
    "#         old_pred_list = MG_exome_merged_df.loc[identity,gene]\n",
    "        \n",
    "#         gene_pred_e_group = list()\n",
    "    \n",
    "    \n",
    "#         #Loop over the two alleles\n",
    "#         for allele_list in old_pred_list:\n",
    "            \n",
    "#             allele_pred_e_group = list()\n",
    "            \n",
    "#             #Convert each prediction to it's respective correct format:\n",
    "#             for allele in allele_list:\n",
    "#                 allele_pred_e_group.append(convert_to_e_group(allele))\n",
    "                                \n",
    "        \n",
    "#             #Merge the two alleles for each gene in a list\n",
    "#             gene_pred_e_group.append(list(set(allele_pred_e_group)))\n",
    "\n",
    "#         #Update dataframes wit0h the new predictions\n",
    "#         gs_e_group_df.at[identity,gene] = gene_pred_e_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('3.10.6': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dd7280d5ac665bc5c6340df06cdd08e38d0895020d34eaa73926ea8f034d08d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
